{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "- Imports\n",
    "- Add Functions\n",
    "- Load Data\n",
    "- Feature Extraction & Data Prep\n",
    "- Remove Multicolinearity\n",
    "- Train Test Split\n",
    "- Encoding\n",
    "- Pipeline & Hyper-parameter Tuning\n",
    "- Feature Selection\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laurent\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning:\n",
      "\n",
      "The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from itertools import combinations \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn import neighbors\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "import scipy.stats as st\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"scripts/functions.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join('data', 'kickstarter_2016_clean.csv')\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction & Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the features to the correct data type\n",
    "df['deadline'] = df['deadline'].astype('datetime64[ns]') \n",
    "df['launched'] = df['launched'].astype('datetime64[ns]') \n",
    "\n",
    "#Feature Extract\n",
    "df['deadline_year'] =  df['deadline'].map(lambda x: x.year)\n",
    "df['deadline_month'] =  df['deadline'].map(lambda x: x.month)\n",
    "df['deadline_day'] =  df['deadline'].map(lambda x: x.day)\n",
    "df['launch_year'] =  df['launched'].map(lambda x: x.year)\n",
    "df['launch_month'] =  df['launched'].map(lambda x: x.month)\n",
    "df['launch_day'] =  df['launched'].map(lambda x: x.day)\n",
    "df['name_word_count'] = df['name'].map(lambda x: len(x.split()))\n",
    "df['name_char_count'] = df['name'].map(lambda x: len(x))\n",
    "\n",
    "\n",
    "#[Fix] Salt of the Earth: A Dead Sea Movie (Canceled)\n",
    "df.iloc[2443][\"launched\"] = df.iloc[2443][\"launched\"].replace(year=2010, month=7, day=23) \n",
    "\n",
    "#[Remove] Could not confirm the rest of the observation where launch year is 1970.\n",
    "df.drop(df[df[\"launch_year\"] < 2009].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VIF values implied correlation between country and currency. Country had a slightly higher VIF. \n",
    "# Currency was chosen and country was dropped.\n",
    "df.drop(columns=['country', 'launched', 'name', 'deadline'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = ['state', 'backers', 'usd_pledged'])\n",
    "y = df['pledged']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ['category', 'main_category', 'currency']\n",
    "encoded_X_train = mean_encoding(X_train, tmp, y.name)\n",
    "encoded_X_test = mean_encoding(X_test, tmp, y.name)\n",
    "encoded_X_train.drop(columns=['pledged'], inplace=True)\n",
    "encoded_X_test.drop(columns=['pledged'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Train and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "(encoded_X_train\n",
    "    .merge(df[['state', 'backers', 'usd_pledged', 'pledged']], left_index=True, right_index=True)\n",
    "    .to_csv('data/kickstarter_train_2016_prep.csv', index=False)\n",
    ")\n",
    "\n",
    "(encoded_X_test\n",
    "    .merge(df[['state', 'backers', 'usd_pledged', 'pledged']], left_index=True, right_index=True)\n",
    "    .to_csv('data/kickstarter_test_2016_prep.csv', index=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline & Hyper-parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "\n",
    "# Create a pipeline\n",
    "pipe = Pipeline([('reg', DummyEstimator())]) # Placeholder Estimator\n",
    "\n",
    "# Candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'reg': [XGBRegressor()], # Actual Estimator\n",
    "                 'reg__eta': [0.2, 0.3, 0.7, 0.9],\n",
    "                 'reg__max_depth': [6, 10, 17],\n",
    "                 'reg__max_iter': [100, 200, 300]},\n",
    "                \n",
    "                {'reg':[GradientBoostingRegressor()], # Actual Estimator\n",
    "                 'reg__learning_rate': [0.1, 0.2, 0.4],  #Parameters\n",
    "                 'reg__n_estimators': [100, 120, 150]\n",
    "                },\n",
    "                \n",
    "                {'reg': [RandomForestRegressor()],\n",
    "                 'reg__n_estimators' : [25, 50, 100],\n",
    "                 'reg__max_depth' : [None, 40, 100],\n",
    "                 'reg__oob_score' :[True]},             \n",
    "               ]\n",
    "\n",
    "\n",
    "# Create grid search \n",
    "gs = GridSearchCV(pipe, search_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(encoded_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = 'semi_final_model.pkl'\n",
    "\n",
    "#Save model\n",
    "with open(model_file_name, 'wb') as out:\n",
    "    pickle.dump(gs.best_estimator_, out)\n",
    "    \n",
    "#Read Model\n",
    "#with open(model_file_name , 'rb') as inp:\n",
    "#    clf = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = permutation_importance(model, X, y, scoring='neg_mean_squared_error')\n",
    "importance = results.importances_mean\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Box-Cox transform and the Yeo-Johnson transform](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)\n",
    "- [Box vs Yeo](https://stats.stackexchange.com/questions/430419/box-cox-vs-yeo-johnson)\n",
    "- [Normalizing Guidelines]((https://stackoverflow.com/questions/49444262/normalize-data-before-or-after-split-of-training-and-testing-data#:~:text=You%20first%20need%20to%20split,set%20could%20be%20useful%20too).&text=Therefore%2C%20you%20should%20perform%20feature,variance%20of%20training%20explanatory%20variables.)\n",
    "- [Rate per capital](https://www.robertniles.com/stats/percap.shtml)\n",
    "- [Penalize Regression with categorical](https://stats.stackexchange.com/questions/359015/ridge-lasso-standardization-of-dummy-indicators)\n",
    "- [Feature Importance](https://medium.com/bigdatarepublic/feature-importance-whats-in-a-name-79532e59eea3)\n",
    "- [Better feature importance](https://machinelearningmastery.com/calculate-feature-importance-with-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
